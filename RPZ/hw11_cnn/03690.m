% Init MatConvNet framework
MCNPath = './matconvnet-1.0-beta23/matconvnet-1.0-beta23';
run(fullfile(MCNPath, 'matlab/vl_setupnn'))

% load data
load imdb

% for augmentation it is easier to have the original data
imdb.images.data(:,:,1,:) = imdb.images.data(:,:,1,:) + imdb.images.data_mean;

% SCREW THE IMAGES
for i = 1:59000
    imdb.images.data(:,:,1,i) = reshape(screw_image(reshape(imdb.images.data(:,:,1,i), [28,28])), [28,28,1]);
end
imdb.images.data_mean = mean(imdb.images.data, 4);
imdb.images.data = (imdb.images.data - imdb.images.data_mean) / 255;

my_interval = 1:18;
sample_interval = 59201:59212;
figure 
colormap(gray)  
for i = my_interval 
    subplot(5,6,i) 
    digit = reshape(imdb.images.data(:,:,1,i), [28,28]);
    imagesc(digit)
    title(num2str(imdb.images.labels(i)))
end
for i = sample_interval
    subplot(5,6,i-59200+18)                              
    digit = reshape(imdb.images.data(:,:,1,i), [28,28]);   
    imagesc(digit)                           
    title(num2str(imdb.images.labels(i)))
end

% training and validation sets
imdb.images.set = [1 * ones(1, 59000), 2 * ones(1, 1000)];

%% 2. CNNs
% --------------------------------------------------------------------
% adding layers, dropout and max-pooling
clear net;
delete expDir/*

net.layers = {} ;
%conv1
net.layers{end+1} = struct('name', 'conv1', ...
			   'type', 'conv', ...
			   'weights', {{1*randn(3,3,1,32,'single'), zeros(1, 32,'single')}}, ...
			   'stride', 2, ...
			   'pad', 1) ;
net.layers{end+1} = struct('name', 'relu1', ...
			   'type', 'relu') ;
net.layers{end+1} = struct('name', 'conv1', ...
			   'type', 'conv', ...
			   'weights', {{(1/32)*randn(3,3,32,64,'single'), zeros(1, 64,'single')}}, ...
			   'stride', 1, ...
			   'pad', 0) ;
net.layers{end+1} = struct('name', 'relu1', ...
			   'type', 'relu') ;           
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0);
net.layers{end+1} = struct('name', 'conv1', ...
			   'type', 'conv', ...
			   'weights', {{(1/64)*randn(3,3,64,512,'single'), zeros(1, 512,'single')}}, ...
			   'stride', 1, ...
			   'pad', 0) ;
net.layers{end+1} = struct('name', 'relu1', ...
			   'type', 'relu') ;      
net.layers{end+1} = struct('name', 'conv1', ...
			   'type', 'conv', ...
			   'weights', {{(1/512)*randn(4,4,512,512,'single'), zeros(1, 512,'single')}}, ...
			   'stride', 1, ...
			   'pad', 0) ;
net.layers{end+1} = struct('name', 'relu1', ...
			   'type', 'relu') ;  
net.layers{end+1} = struct('name', 'full', ...
			   'type', 'conv', ...
			   'weights', {{(1/512)*randn(1,1,512,10,'single'), zeros(1, 10,'single')}}, ...
			   'stride', 1, ...
			   'pad', 0);        
net.layers{end+1} = struct('type', 'softmaxloss') ;


net = vl_simplenn_tidy(net);
vl_simplenn_display(net)

[net, info] = cnn_train(net, imdb, @getSimpleNNBatch, 'batchSize', 1000, 'numEpochs', 15, 'expDir', 'expDir', 'learningRate',0.07);   %'plotStatistics', false);
% save your best network
net.layers{end}.type = 'softmax';
save('my_cnn.mat', 'net');
