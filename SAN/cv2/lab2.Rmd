---
title: "SAN 2nd lab - T-test crash course"
date: "30/09/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In this lab, we will be taking a fast track through t-test, central limit theorem and confidence intervals. The lab leaves many terms
unexplained (e.g. t-distribuion, sampling distribution, etc.), as it focuses mainly on the application of said terms. For further understanding or recap, please refer to additional sources (some will be listed below). 

### T-test practical examples
Assume researches are interested in sleeping habits of high school students and college students. Below are samples of the two groups
and the task is to determine, if there is indeed a statistical difference in hours slept. 
```{r}
# Two sample independent test
high_school <- c(7.0, 7.1, 6.7, 7.7, 7.3, 6.3, 7.0, 6.6, 7.2, 6.6)
college <- c(6.8, 6.0, 6.3, 6.9, 6.8, 6.1, 6.7, 6.6, 6.3, 6.5)
t.test(high_school, college)
```
For now ignore most of the summary and focus only on the alternative hypothesis and the p-value. What conclusions can we draw?
Let's see other versions of the test:
```{r}
# One sample test
# In this setting, we assume something about the population (here that high schoolers sleep on average 7 hours)
# And we test, whether it corresponds with the reality. 
high_school <- c(7.0, 7.1, 6.7, 7.7, 7.3, 6.3, 7.0, 6.6, 7.2, 6.6)
expected_hours <- 7
t.test(high_school, mu = expected_hours)

# Paired test
# In paired test, there is only one group of students, which are however measured twice - once during high school 
# and second time on college
t.test(high_school, college, paired = TRUE)
```
In pair test, compare the p-value with the p-value from the two sample independent test. You wil notice it is significantly smaller even 
though we used the same data. This is because a great portion of variance in the data is "absorbed" by the pair-wise nature of the test (if Greg is sleeping a lot during high school, he will probably sleep a lot too during college - in the independent test this would result in having 2 extreme values (causing higher variance), in the paired test however, the extreme values are linked and cancel each other). 

### Hypothesis testing - bartender example
Assume we have a bartender we are suspecting from selling below average amount of beer. We therefore proceed to take samples of his beers. 

```{r}
# We have a beer sample of our suspected bartender
beer_samples <- c(490.5, 472.3, 475.8, 492.2, 459.9, 522.9, 487.5, 491.6, 479.7, 467.2, 469.8, 495.2, 459.3, 488.3, 468.9, 465.6, 479.0, 476.8, 493.3, 487.7, 473.9, 461.1, 475.9, 491.3, 476.9, 486.2, 482.9, 464.6, 460.9, 504.1, 498.9, 454.2, 502.8, 489.8, 527.7, 492.2, 477.2, 486.5, 478.4, 482.4, 508.5, 490.9, 473.2, 481.2, 514.3)
```

Our null hypothesis is that he's an OK bartender with on average 500ml beer servings. So let's see, if our bartender is close to the
OK one.

```{r}
plot(beer_samples)
# Visualize the samples as hist, hist probab and probability density
hist(beer_samples)
plot(density(beer_samples))

# Now plot both the sample and expected means (the later in red)
abline(v = 500, lw = 2, col="red")
abline(v = mean(beer_samples), lw = 2)
```

We see that our bartender's mean is less, than the expected 500 - we have some evidence against the assumption, he's a fair bartender.
But how do we evaluate the strenght of the evidence?

If you recall the example with cheating brother - we do it by calculating the probability of observing the "event"/"result", given he is not cheating, i.e. in mathematical notation $P(\text{washing dishes 4 and more times in a row} | \text{brother not cheating})$. This is the **probability of seeing the observation assuming the null hypothesis is true** $P(o|H_0)$. 

We do the same here, just for another distribution. We assume the amounts of beer have *normal distribution* and calculate the probability we got beer samples (having on average cca 480ml), assuming the bartender is indeed fair. Recall again, that we compare this probability with the threshold of 5% to decide, whether we should reject the null hypothesis. We basically say **the event is too rare under the null hypothesis** and something might be fishy here. [^1]

[^1]: Note here, that we are dealing with distribution of *sample means* (or *sampling distributions*). This topic has not been addressed for there was no room in the labs. To get the thorough understanding of hypothesis testing, please revise additional materials. For instance at \url{https://www.spss-tutorials.com/sampling-distribution-what-is-it/}

In this case we don't know all the parameters of the true distribution - we know the mean but **we don't the standard deviation of the "fair" bartender**. We however know the mean and sd of our tested bartender. It is therefore possible to ask an equivalent question - *how probable is that we get beer samples having on average 0.5l+ with our tested bartender*. If this chance is small, his actual beer performance will be most likely less, than 0.5l. 

```{r}
# What is the p-value of the H0?
t.test(beer_samples, mu = 500)
```
The t-score above together with df (*degrees for freedom*) lead to the final p-value we are interested in.
We can say, that how probable the true mean is 500ml (given the beer samples of our barterner) is calculated from the t-score:
$$t= \frac{\overline{x} - \mu}{\sigma_{\overline{x}}} =  \frac{\overline{x} - \mu}{\sigma/\sqrt{n}} = \frac{\text{bartender mean} - \text{expected mean} }{\text{bartender sampling standard deviation}}$$
where $\sigma_{\overline{x}}$ is standart deviation of the sampling distribution, which can be derived from population sd of samples $\sigma$ "adjusted" by the number of samples $n$.  
This formula basically tells, how many sampling standard deviations fit into the difference between the two means. If the difference is too big in terms of the deviation, we reject the null hypothesis. You can try to fill in the respective values to see, if you get the same t-score as in the previous test.

## Central limit theorem
T-tests build on the idea of a so called *central limit theorem*. The reason why we use normal (or t-) distribution in the tests spring from the implications of the said theorem.

What CLM says?. In simple terms, it says: take a bunch of samples from a random distribution (choose any you want). Then calculate a mean of each sample - now you have a bunch of means. And the statement here is - those means will have approximately a nomal distibution as the sample size approaches infinity. 

So let's do exacty that. Pick an arbitrary distribution (here a uniform one). Genereate samples from it and calculate their means. Plot the means to histogram. Does it resemble a normal distribution? 

```{r}
central_theorem_demo <- function(sample_size, measurements, min, max) {
  rand_samples <- matrix(runif(measurements * sample_size, min, max),
                       nrow = measurements)
  
   # If you choose to use a binomail distribution, uncomment following lines
   # rand_samples = matrix(
   #   rbinom(measurements * sample_size, max, 0.4),
   #   nrow = measurements
   # )

  # First, observe how our data look like.
  # Notice that they don't need to be normally distributed for the theorem to work
  # hist(rand_samples[1,])  # uncomment to see
  
  # Calculate sample means for all measurements
  means <- apply(rand_samples, MARGIN = 1, mean)
  
  # Plot the means distribution. And now magic happens. Does it look like a normal one?
  # Note this is not the distribution of our datapoints, but the distribution of means of each sample measurements
  h <- hist(means, breaks = 50, probability = TRUE,)
  
  # Draw the supposed normal distribution curve. Note that for small sample sizes
  # the sample distribution will not correspond to the curve - the CLT does not hold
  # Generate x-axis as evenly spread values (more values the smoother the line)
  x <- seq(from = min, to = max, length = 1000)
  # Calculate the corresponding probability densities (according to normal distr) 
  y <- dnorm(
    x,
    mean = mean(rand_samples),
    sd = sd(rand_samples) / sqrt(sample_size)
  )
  
  lines(x, y, col = "black", lwd = 2)
}
# -----------------------------------------------------------------------------
measurements <- 1000
## Increase the number of samples to see the effect. What happens when you set 1
sample_size <- 10

central_theorem_demo(sample_size, measurements, 0, 100)
```

Recall the example on hypothesis testing with the bartender. We kind of assumed there, that the means of beer samples have normal distribution. **With the CLT, we now know, that the beer samples indeed had a normal distribution.**

### Confidence intervals using central limit theorem
From the CLT we know, that a mean of our measured sample can be a reliable estimation of the true mean. Since the sample mean is however random (as samples are random), we need to somehow have a measure of how good the estimate is. We need to measure the "precision" or "confidence" of the estimate. 

*Confidence intervals* are serving exactly that purpose. They are calculated from the sample we've taken and set a range of values around the measured mean and say: "With some probability (usually 95percent) I'm sure, that the true mean is somewhere inside this interval". Or in other words - if I were to do the measuremnts 100 times (take 100 samples), in 95% of cases the constructed confidence interval will capture the true mean. 

Caution! We are not saying: "The true mean is with 95% probability inside the interval". The confidence intervals are random (they are calculated from *random* samples)! Don't forget, that the true means is **fixed** and it's confidence intervals, that "move around" it.
Sometimes we can get "bad sample" totally unrepresentative of the population and the mean will diverge from the true one. In that case the corresponding confidence interval will not capture the true mean.

Let's have a simulation. We will take 100 samples (heights of men), construct 100 confidence intervals and see, how many of them will capture the true mean. 
```{r}
# ----------------------- helper function -------------------------------------
# Define a function, that constructs confidence interval given a sample
get_conf_interval <- function(pop_sample) {
  sample_size <- length(pop_sample)
  sample_mean <- mean(pop_sample)
  sample_sd <- sd(pop_sample)
  
  population_sd_estimate <- sample_sd / sqrt(sample_size)
  
  # Recall probability mass distribution of the "Gaussian"
  interval_start <-  sample_mean - 2 * population_sd_estimate
  interval_end <- sample_mean + 2 * population_sd_estimate
  
  print(c(interval_start, interval_end))
  
  return(
    data.frame(
      mean = sample_mean,
      interval_start = interval_start,
      interval_end  =interval_end
    )
  )
}
# -----------------------------------------------------------------------------
# Construct a sample
sample_size <- 100
# The parameters of the true distribution. Samples are generated from this distribution 
true_mean <- 173
true_sd <- 20
# Sample the distribution
pop_sample <- rnorm(mean = true_mean, sd = true_sd, n = sample_size)

hist(pop_sample)

# Let's see, how such a confidence interval can look like
interval_params <- get_conf_interval(pop_sample = pop_sample)
abline(v = interval_params$mean, lw = 2)
# Plot the true mean alongside with the population mean. 
abline(v = true_mean, lw = 2, col="red")
# Plot the interval. Is the true mean inside the dash lines?
abline(v = interval_params$interval_start, lty = 2, lw = 2)
abline(v = interval_params$interval_end, lty = 2, lw = 2)

# -----------------------------------------------------------------------------
# Now let's take 100 samples and construct 100 95% confidence intervals.
# These intervals serve as estimations of the true mean 
no_experiments <- 100
future_experiments = matrix(rnorm(
  mean = true_mean,
  sd = true_sd,
  n = no_experiments * sample_size
),
nrow = no_experiments)

# View(future_experiments)

# In how many of them we can find the true mean?
print("Calculate 100 95% confidence intervals. The numbers are in format 'interval_start' and 'interval_end'")
results <- apply(
  future_experiments,
  MARGIN = 1,
  FUN = function(row) {
    interval_params <- get_conf_interval(pop_sample = row)
    interval_params$interval_start < true_mean && true_mean < interval_params$interval_end
  }
)

# Let see, how many of the 100 95% confidence intervals didn't capture the true mean.
# The number should be by design close to 5%
print("Number of 'wrong' 95% confidence intervals:")
print(length(results[results == FALSE]))
```
